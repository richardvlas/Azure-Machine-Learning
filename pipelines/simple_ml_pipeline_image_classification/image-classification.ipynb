{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple ML pipeline for image classification\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the [Fashion MNIST dataset and Keras on Azure Machine Learning. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1618857515040
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.38.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import (\n",
    "    Workspace,\n",
    "    Dataset,\n",
    "    Datastore,\n",
    "    ComputeTarget,\n",
    "    Experiment,\n",
    "    ScriptRunConfig,\n",
    ")\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1618857518344
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: aml-workspace\n",
      "Azure region: westeurope\n",
      "Subscription id: b17f1c19-34a2-47b8-a207-40ea477828fc\n",
      "Resource group: aml-resource-group\n"
     ]
    }
   ],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print(\n",
    "    \"Workspace name: \" + workspace.name,\n",
    "    \"Azure region: \" + workspace.location,\n",
    "    \"Subscription id: \" + workspace.subscription_id,\n",
    "    \"Resource group: \" + workspace.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1618857519697
    }
   },
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "exp = Experiment(workspace=workspace, name=\"keras-mnist-fashion\")\n",
    "\n",
    "# create a directory\n",
    "script_folder = \"./keras-mnist-fashion\"\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "InProgress....\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\" #\"gpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = workspace.compute_targets\n",
    "if cluster_name in cts and cts[cluster_name].type == \"AmlCompute\":\n",
    "    found = True\n",
    "    print(\"Found existing compute target.\")\n",
    "    compute_target = cts[cluster_name]\n",
    "if not found:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_D12_v2\",  # for GPU, use \"STANDARD_NC6\"\n",
    "        vm_priority = 'lowpriority', # optional\n",
    "        max_nodes=4,\n",
    "    )\n",
    "    \n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=10\n",
    "    )\n",
    "# For a more detailed view of current AmlCompute status, use get_status().print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Fashion MNIST dataset\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1618857745715
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnist-fashion/t10k-images-idx3-ubyte',\n",
       " '/mnist-fashion/t10k-labels-idx1-ubyte',\n",
       " '/mnist-fashion/train-images-idx3-ubyte',\n",
       " '/mnist-fashion/train-labels-idx1-ubyte']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_urls = [\"https://data4mldemo6150520719.blob.core.windows.net/demo/mnist-fashion\"]\n",
    "fashion_ds = Dataset.File.from_files(data_urls)\n",
    "\n",
    "# list the files referenced by fashion_ds\n",
    "fashion_ds.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-step ML pipeline\n",
    "\n",
    "### Step 1: data preparation\n",
    "\n",
    "In step one, we will load the image and labels from Fashion MNIST dataset into mnist_train.csv and mnist_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data (or output of a step) is represented by a `OutputFileDatasetConfig` object. preprared_fashion_ds is produced as the output of step 1, and used as the input of step 2. `OutputFileDatasetConfig` introduces a data dependency between steps, and creates an implicit execution order in the pipeline. You can register a `OutputFileDatasetConfig` as a dataset and version the output data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1618857745896
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OutputFileDatasetConfig in module azureml.data.output_dataset_config:\n",
      "\n",
      "class OutputFileDatasetConfig(OutputDatasetConfig, TransformationMixin)\n",
      " |  OutputFileDatasetConfig(name=None, destination=None, source=None, partition_format=None)\n",
      " |  \n",
      " |  Represent how to copy the output of a run and be promoted as a FileDataset.\n",
      " |  \n",
      " |  The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target\n",
      " |  to be uploaded to the specified destination. If no arguments are passed to the constructor, we will\n",
      " |  automatically generate a name, a destination, and a local path.\n",
      " |  \n",
      " |  An example of not passing any arguments:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      workspace = Workspace.from_config()\n",
      " |      experiment = Experiment(workspace, 'output_example')\n",
      " |  \n",
      " |      output = OutputFileDatasetConfig()\n",
      " |  \n",
      " |      script_run_config = ScriptRunConfig('.', 'train.py', arguments=[output])\n",
      " |  \n",
      " |      run = experiment.submit(script_run_config)\n",
      " |      print(run)\n",
      " |  \n",
      " |  An example of creating an output then promoting the output to a tabular dataset and register it with\n",
      " |  name foo:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      workspace = Workspace.from_config()\n",
      " |      experiment = Experiment(workspace, 'output_example')\n",
      " |  \n",
      " |      datastore = Datastore(workspace, 'example_adls_gen2_datastore')\n",
      " |  \n",
      " |      # for more information on the parameters and methods, please look for the corresponding documentation.\n",
      " |      output = OutputFileDatasetConfig().read_delimited_files().register_on_complete('foo')\n",
      " |  \n",
      " |      script_run_config = ScriptRunConfig('.', 'train.py', arguments=[output])\n",
      " |  \n",
      " |      run = experiment.submit(script_run_config)\n",
      " |      print(run)\n",
      " |  \n",
      " |  .. remarks::\n",
      " |      You can pass the OutputFileDatasetConfig as an argument to your run, and it will be automatically\n",
      " |      translated into local path on the compute. The source argument will be used if one is specified,\n",
      " |      otherwise we will automatically generate a directory in the OS's temp folder. The files and folders inside\n",
      " |      the source directory will then be copied to the destination based on the output configuration.\n",
      " |  \n",
      " |      By default the mode by which the output will be copied to the destination storage will be set to mount.\n",
      " |      For more information about mount mode, please see the documentation for as_mount.\n",
      " |  \n",
      " |  :param name: The name of the output specific to this run. This is generally used for lineage purposes. If set\n",
      " |      to None, we will automatically generate a name. The name will also become an environment variable which\n",
      " |      contains the local path of where you can write your output files and folders to that will be uploaded to\n",
      " |      the destination.\n",
      " |  :type name: str\n",
      " |  :param destination: The destination to copy the output to. If set to None, we will copy the output to the\n",
      " |      workspaceblobstore datastore, under the path /dataset/{run-id}/{output-name}, where `run-id` is the Run's\n",
      " |      ID and the `output-name` is the output name from the `name` parameter above. The destination is a tuple\n",
      " |      where the first item is the datastore and the second item is the path within the datastore to copy the\n",
      " |      data to.\n",
      " |  \n",
      " |      The path within the datastore can be a template path. A template path is just a regular path but with\n",
      " |      placeholders inside. Those placeholders will then be resolved at the appropriate time. The syntax for\n",
      " |      placeholders is {placeholder}, for example, /path/with/{placeholder}. Currently only two placeholders\n",
      " |      are supported, {run-id} and {output-name}.\n",
      " |  :type destination: tuple\n",
      " |  :param source: The path within the compute target to copy the data from. If set to None, we\n",
      " |      will set this to a directory we create inside the compute target's OS temporary directory.\n",
      " |  :type source: str\n",
      " |  :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../Accounts/2019/01/01/data.parquet' where the partition is by\n",
      " |          department name and time, partition_format='/{Department}/{PartitionDate:yyyy/MM/dd}/data.parquet'\n",
      " |          creates a string column 'Department' with the value 'Accounts' and a datetime column 'PartitionDate'\n",
      " |          with the value '2019-01-01'.\n",
      " |  :type partition_format: str\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OutputFileDatasetConfig\n",
      " |      OutputDatasetConfig\n",
      " |      TransformationMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name=None, destination=None, source=None, partition_format=None)\n",
      " |      Initialize a OutputFileDatasetConfig.\n",
      " |      \n",
      " |      The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target\n",
      " |      to be uploaded to the specified destination. If no arguments are passed to the constructor, we will\n",
      " |      automatically generate a name, a destination, and a local path.\n",
      " |      \n",
      " |      An example of not passing any arguments:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          workspace = Workspace.from_config()\n",
      " |          experiment = Experiment(workspace, 'output_example')\n",
      " |      \n",
      " |          output = OutputFileDatasetConfig()\n",
      " |      \n",
      " |          script_run_config = ScriptRunConfig('.', 'train.py', arguments=[output])\n",
      " |      \n",
      " |          run = experiment.submit(script_run_config)\n",
      " |          print(run)\n",
      " |      \n",
      " |      An example of creating an output then promoting the output to a tabular dataset and register it with\n",
      " |      name foo:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          workspace = Workspace.from_config()\n",
      " |          experiment = Experiment(workspace, 'output_example')\n",
      " |      \n",
      " |          datastore = Datastore(workspace, 'example_adls_gen2_datastore')\n",
      " |      \n",
      " |          # for more information on the parameters and methods, please look for the corresponding documentation.\n",
      " |          output = OutputFileDatasetConfig().read_delimited_files().register_on_complete('foo')\n",
      " |      \n",
      " |          script_run_config = ScriptRunConfig('.', 'train.py', arguments=[output])\n",
      " |      \n",
      " |          run = experiment.submit(script_run_config)\n",
      " |          print(run)\n",
      " |      \n",
      " |      .. remarks::\n",
      " |          You can pass the OutputFileDatasetConfig as an argument to your run, and it will be automatically\n",
      " |          translated into local path on the compute. The source argument will be used if one is specified,\n",
      " |          otherwise we will automatically generate a directory in the OS's temp folder. The files and folders inside\n",
      " |          the source directory will then be copied to the destination based on the output configuration.\n",
      " |      \n",
      " |          By default the mode by which the output will be copied to the destination storage will be set to mount.\n",
      " |          For more information about mount mode, please see the documentation for as_mount.\n",
      " |      \n",
      " |      :param name: The name of the output specific to this run. This is generally used for lineage purposes. If set\n",
      " |          to None, we will automatically generate a name. The name will also become an environment variable which\n",
      " |          contains the local path of where you can write your output files and folders to that will be uploaded to\n",
      " |          the destination.\n",
      " |      :type name: str\n",
      " |      :param destination: The destination to copy the output to. If set to None, we will copy the output to the\n",
      " |          workspaceblobstore datastore, under the path /dataset/{run-id}/{output-name}, where `run-id` is the Run's\n",
      " |          ID and the `output-name` is the output name from the `name` parameter above. The destination is a tuple\n",
      " |          where the first item is the datastore and the second item is the path within the datastore to copy the\n",
      " |          data to.\n",
      " |      \n",
      " |          The path within the datastore can be a template path. A template path is just a regular path but with\n",
      " |          placeholders inside. Those placeholders will then be resolved at the appropriate time. The syntax for\n",
      " |          placeholders is {placeholder}, for example, /path/with/{placeholder}. Currently only two placeholders\n",
      " |          are supported, {run-id} and {output-name}.\n",
      " |      :type destination: tuple\n",
      " |      :param source: The path within the compute target to copy the data from. If set to None, we\n",
      " |          will set this to a directory we create inside the compute target's OS temporary directory.\n",
      " |      :type source: str\n",
      " |      :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../Accounts/2019/01/01/data.parquet' where the partition is by\n",
      " |          department name and time, partition_format='/{Department}/{PartitionDate:yyyy/MM/dd}/data.parquet'\n",
      " |          creates a string column 'Department' with the value 'Accounts' and a datetime column 'PartitionDate'\n",
      " |          with the value '2019-01-01'.\n",
      " |      :type partition_format: str\n",
      " |  \n",
      " |  as_input(self, name=None)\n",
      " |      Specify how to consume the output as an input in subsequent pipeline steps.\n",
      " |      \n",
      " |      :param name: The name of the input specific to the run.\n",
      " |      :type name: str\n",
      " |      :return: A :class:`azureml.data.dataset_consumption_config.DatasetConsumptionConfig` instance describing\n",
      " |          how to deliver the input data.\n",
      " |      :rtype: azureml.data.dataset_consumption_config.DatasetConsumptionConfig\n",
      " |  \n",
      " |  as_mount(self, disable_metadata_cache=False)\n",
      " |      Set the mode of the output to mount.\n",
      " |      \n",
      " |      For mount mode, the output directory will be a FUSE mounted directory. Files written to the mounted directory\n",
      " |      will be uploaded when the file is closed.\n",
      " |      \n",
      " |      :param disable_metadata_cache: Whether to cache metadata in local node,\n",
      " |          if disabled a node will not be able to see files generated from other nodes during job running.\n",
      " |      :type disable_metadata_cache: bool\n",
      " |      :return: A :class:`azureml.data.OutputFileDatasetConfig` instance with mode set to mount.\n",
      " |      :rtype: azureml.data.OutputFileDatasetConfig\n",
      " |  \n",
      " |  as_upload(self, overwrite=False, source_globs=None)\n",
      " |      Set the mode of the output to upload.\n",
      " |      \n",
      " |      For upload mode, files written to the output directory will be uploaded at the end of the job. If the job\n",
      " |      fails or gets canceled, then the output directory will not be uploaded.\n",
      " |      \n",
      " |      :param overwrite: Whether to overwrite files that already exists in the destination.\n",
      " |      :type overwrite: bool\n",
      " |      :param source_globs: Glob patterns used to filter files that will be uploaded.\n",
      " |      :type source_globs: builtin.list[str]\n",
      " |      :return: A :class:`azureml.data.OutputFileDatasetConfig` instance with mode set to upload.\n",
      " |      :rtype: azureml.data.OutputFileDatasetConfig\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from OutputDatasetConfig:\n",
      " |  \n",
      " |  register_on_complete(self, name, description=None, tags=None)\n",
      " |      Register the output as a new version of a named Dataset after the run has ran.\n",
      " |      \n",
      " |      If there are no datasets registered under the specified name, a new Dataset with the specified name will be\n",
      " |      registered. If there is a dataset registered under the specified name, then a new version will be added\n",
      " |      to this dataset.\n",
      " |      \n",
      " |      :param name: The Dataset name to register the output under.\n",
      " |      :type name: str\n",
      " |      :param description: The description for the Dataset.\n",
      " |      :type description: str\n",
      " |      :param tags: A list of tags to be assigned to the Dataset.\n",
      " |      :type tags: dict[str, str]\n",
      " |      :return: A new :class:`azureml.data.output_dataset_config.OutputDatasetConfig` instance with the registration\n",
      " |          information.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputDatasetConfig\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OutputDatasetConfig:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of the output.\n",
      " |      \n",
      " |      :return: Name of the output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from TransformationMixin:\n",
      " |  \n",
      " |  read_delimited_files(self, include_path=False, separator=',', header=<PromoteHeadersBehavior.ALL_FILES_HAVE_SAME_HEADERS: 3>, partition_format=None, path_glob=None, set_column_types=None)\n",
      " |      Transform the output dataset to a tabular dataset by reading all the output as delimited files.\n",
      " |      \n",
      " |      :param include_path: Boolean to keep path information as column in the dataset. Defaults to False.\n",
      " |          This is useful when reading multiple files, and want to know which file a particular record\n",
      " |          originated from, or to keep useful information in file path.\n",
      " |      :type include_path: bool\n",
      " |      :param separator: The separator used to split columns.\n",
      " |      :type separator: str\n",
      " |      :param header: Controls how column headers are promoted when reading from files. Defaults to assume\n",
      " |          that all files have the same header.\n",
      " |      :type header: azureml.data.dataset_type_definitions.PromoteHeadersBehavior\n",
      " |      :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../Accounts/2019/01/01/data.parquet' where the partition is by\n",
      " |          department name and time, partition_format='/{Department}/{PartitionDate:yyyy/MM/dd}/data.parquet'\n",
      " |          creates a string column 'Department' with the value 'Accounts' and a datetime column 'PartitionDate'\n",
      " |          with the value '2019-01-01'.\n",
      " |      :type partition_format: str\n",
      " |      :param path_glob: A glob pattern to filter files that will be read as delimited files. If set to None, then\n",
      " |          all files will be read as delimited files.\n",
      " |      :type path_glob: str\n",
      " |      :param set_column_types: A dictionary to set column data type, where key is column name and value is\n",
      " |          :class:`azureml.data.DataType`. Columns not in the dictionary will remain of type string. Passing None\n",
      " |          will result in no conversions. Entries for columns not found in the source data will not cause an error\n",
      " |          and will be ignored.\n",
      " |      :type set_column_types: dict[str, azureml.data.DataType]\n",
      " |      :return: A :class:`azureml.data.output_dataset_config.OutputTabularDatasetConfig` instance with instruction of\n",
      " |          how to convert the output into a TabularDataset.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputTabularDatasetConfig\n",
      " |  \n",
      " |  read_parquet_files(self, include_path=False, partition_format=None, path_glob=None, set_column_types=None)\n",
      " |      Transform the output dataset to a tabular dataset by reading all the output as Parquet files.\n",
      " |      \n",
      " |      The tabular dataset is created by parsing the parquet file(s) pointed to by the intermediate output.\n",
      " |      \n",
      " |      :param include_path: Boolean to keep path information as column in the dataset. Defaults to False.\n",
      " |          This is useful when reading multiple files, and want to know which file a particular record\n",
      " |          originated from, or to keep useful information in file path.\n",
      " |      :type include_path: bool\n",
      " |      :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../Accounts/2019/01/01/data.parquet' where the partition is by\n",
      " |          department name and time, partition_format='/{Department}/{PartitionDate:yyyy/MM/dd}/data.parquet'\n",
      " |          creates a string column 'Department' with the value 'Accounts' and a datetime column 'PartitionDate'\n",
      " |          with the value '2019-01-01'.\n",
      " |      :type partition_format: str\n",
      " |      :param path_glob: A glob pattern to filter files that will be read as parquet files. If set to None, then\n",
      " |          all files will be read as parquet files.\n",
      " |      :type path_glob: str\n",
      " |      :param set_column_types: A dictionary to set column data type, where key is column name and value is\n",
      " |          :class:`azureml.data.DataType`. Columns not in the dictionary will remain of type loaded from the parquet\n",
      " |          file. Passing None will result in no conversions. Entries for columns not found in the source data will\n",
      " |          not cause an error and will be ignored.\n",
      " |      :type set_column_types: dict[str, azureml.data.DataType]\n",
      " |      :return: A :class:`azureml.data.output_dataset_config.OutputTabularDatasetConfig` instance with instruction of\n",
      " |          how to convert the output into a TabularDataset.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputTabularDatasetConfig\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "# learn more about the output config\n",
    "help(OutputFileDatasetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1618857751426
    }
   },
   "outputs": [],
   "source": [
    "# write output to datastore under folder `outputdataset` and register it as a dataset after the experiment completes\n",
    "# make sure the service principal in your datastore has blob data contributor role in order to write data back\n",
    "datastore = workspace.get_default_datastore()\n",
    "prepared_fashion_ds = OutputFileDatasetConfig(\n",
    "    destination=(datastore, \"outputdataset/{run-id}\")\n",
    ").register_on_complete(name=\"prepared_fashion_ds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1618857756352
    }
   },
   "outputs": [],
   "source": [
    "prep_step = PythonScriptStep(\n",
    "    name=\"prepare step\",\n",
    "    script_name=\"prepare.py\",\n",
    "    # mount fashion_ds dataset to the compute_target\n",
    "    arguments=[fashion_ds.as_named_input(\"fashion_ds\").as_mount(), prepared_fashion_ds],\n",
    "    source_directory=script_folder,\n",
    "    compute_target=compute_target,\n",
    "    allow_reuse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in prepare.py takes two command-line arguments: the first is assigned to mounted_input_path and the second to mounted_output_path. If that subdirectory doesn't exist, the call to os.makedirs creates it. Then, the program converts the training and testing data and outputs the comma-separated files to the mounted_output_path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: train CNN with Keras\n",
    "\n",
    "Next, construct a ScriptRunConfig to configure the training run that trains a CNN model using Keras. It takes a dataset as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conda_dependencies.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-core\n",
    "  - azureml-dataset-runtime\n",
    "  - keras==2.4.3\n",
    "  - tensorflow==2.4.3\n",
    "  - numpy\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1618857796171
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "keras_env = Environment.from_conda_specification(\n",
    "    name=\"keras-env\", file_path=\"./conda_dependencies.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1618857800650
    }
   },
   "outputs": [],
   "source": [
    "train_src = ScriptRunConfig(\n",
    "    source_directory=script_folder,\n",
    "    script=\"train.py\",\n",
    "    compute_target=compute_target,\n",
    "    environment=keras_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the run configuration details into the PythonScriptStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1618857803377
    }
   },
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "    name=\"train step\",\n",
    "    arguments=[\n",
    "        prepared_fashion_ds.read_delimited_files().as_input(name=\"prepared_fashion_ds\")\n",
    "    ],\n",
    "    source_directory=train_src.source_directory,\n",
    "    script_name=train_src.script,\n",
    "    runconfig=train_src.run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the pipeline.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using `submit`. When submit is called, a PipelineRun is created which in turn creates StepRun objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step prepare step [48cfbf80][506c8be5-48d7-48af-844a-dc90de8ae3b7], (This step will run and generate new outputs)\n",
      "Created step train step [0cf4bb29][05fccfd1-52e9-40fe-a1c5-ed8715fdffd4], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun f329cb80-6975-47a5-915c-706f71b5f836\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f329cb80-6975-47a5-915c-706f71b5f836?wsid=/subscriptions/b17f1c19-34a2-47b8-a207-40ea477828fc/resourcegroups/aml-resource-group/workspaces/aml-workspace&tid=0f823349-2c12-431b-a03c-b2c0a43d6fb4\n"
     ]
    }
   ],
   "source": [
    "# build pipeline & run experiment\n",
    "pipeline = Pipeline(workspace, steps=[prep_step, train_step])\n",
    "run = exp.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the PipelineRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: f329cb80-6975-47a5-915c-706f71b5f836\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f329cb80-6975-47a5-915c-706f71b5f836?wsid=/subscriptions/b17f1c19-34a2-47b8-a207-40ea477828fc/resourcegroups/aml-resource-group/workspaces/aml-workspace&tid=0f823349-2c12-431b-a03c-b2c0a43d6fb4\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84?wsid=/subscriptions/b17f1c19-34a2-47b8-a207-40ea477828fc/resourcegroups/aml-resource-group/workspaces/aml-workspace&tid=0f823349-2c12-431b-a03c-b2c0a43d6fb4\n",
      "StepRun( prepare step ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "2022/02/23 21:03:45 Downloading source code...\n",
      "2022/02/23 21:03:46 Finished downloading source code\n",
      "2022/02/23 21:03:46 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2022/02/23 21:03:47 Successfully set up Docker network: acb_default_network\n",
      "2022/02/23 21:03:47 Setting up Docker configuration...\n",
      "2022/02/23 21:03:47 Successfully set up Docker configuration\n",
      "2022/02/23 21:03:47 Logging in to registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "2022/02/23 21:03:48 Successfully logged into 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "2022/02/23 21:03:48 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:03:48 Scanning for dependencies...\n",
      "2022/02/23 21:03:48 Successfully scanned dependencies\n",
      "2022/02/23 21:03:48 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  66.56kB\n",
      "\n",
      "Step 1/21 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf: Pulling from azureml/openmpi3.1.2-ubuntu18.04\n",
      "2f94e549220a: Already exists\n",
      "2b2d92136c10: Pulling fs layer\n",
      "0b412d2669ed: Pulling fs layer\n",
      "805003cfcee6: Pulling fs layer\n",
      "ad877dc53455: Pulling fs layer\n",
      "afa783568628: Pulling fs layer\n",
      "d67cf86adb17: Pulling fs layer\n",
      "82770c3b2808: Pulling fs layer\n",
      "c991e53ceb53: Pulling fs layer\n",
      "afa783568628: Waiting\n",
      "ad877dc53455: Waiting\n",
      "82770c3b2808: Waiting\n",
      "c991e53ceb53: Waiting\n",
      "d67cf86adb17: Waiting\n",
      "0b412d2669ed: Verifying Checksum\n",
      "0b412d2669ed: Download complete\n",
      "805003cfcee6: Verifying Checksum\n",
      "805003cfcee6: Download complete\n",
      "afa783568628: Verifying Checksum\n",
      "afa783568628: Download complete\n",
      "2b2d92136c10: Verifying Checksum\n",
      "2b2d92136c10: Download complete\n",
      "d67cf86adb17: Verifying Checksum\n",
      "d67cf86adb17: Download complete\n",
      "82770c3b2808: Verifying Checksum\n",
      "82770c3b2808: Download complete\n",
      "c991e53ceb53: Verifying Checksum\n",
      "c991e53ceb53: Download complete\n",
      "ad877dc53455: Verifying Checksum\n",
      "ad877dc53455: Download complete\n",
      "2b2d92136c10: Pull complete\n",
      "0b412d2669ed: Pull complete\n",
      "805003cfcee6: Pull complete\n",
      "ad877dc53455: Pull complete\n",
      "afa783568628: Pull complete\n",
      "d67cf86adb17: Pull complete\n",
      "82770c3b2808: Pull complete\n",
      "c991e53ceb53: Pull complete\n",
      "Digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      " ---> 54296612fc48\n",
      "Step 2/21 : USER root\n",
      " ---> Running in 6ae4dca826d9\n",
      "Removing intermediate container 6ae4dca826d9\n",
      " ---> e0523acfd0bb\n",
      "Step 3/21 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 98eee27c216a\n",
      "Removing intermediate container 98eee27c216a\n",
      " ---> 5eec9026f4a4\n",
      "Step 4/21 : WORKDIR /\n",
      " ---> Running in 01cb5349f5d2\n",
      "Removing intermediate container 01cb5349f5d2\n",
      " ---> e4eaf0e031ed\n",
      "Step 5/21 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 8b61a796d83c\n",
      "Step 6/21 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 3fe53664e32d\n",
      "Removing intermediate container 3fe53664e32d\n",
      " ---> f11c74622fff\n",
      "Step 7/21 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> 675d11d68fbe\n",
      "Step 8/21 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in 901dde8ac809\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n",
      "done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "openssl-1.0.2u       | 3.1 MB    |            |   0% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "\n",
      "zlib-1.2.11          | 120 KB    |            |   0% \n",
      "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
      "\n",
      "setuptools-50.3.0    | 891 KB    |            |   0% \n",
      "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
      "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
      "\n",
      "python-3.6.2         | 27.0 MB   |            |   0% \n",
      "python-3.6.2         | 27.0 MB   | ##5        |  26% \n",
      "python-3.6.2         | 27.0 MB   | #####6     |  57% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "\n",
      "xz-5.2.5             | 438 KB    |            |   0% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "\n",
      "pip-20.2.4           | 2.0 MB    |            |   0% \n",
      "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
      "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
      "\n",
      "libffi-3.2.1         | 52 KB     |            |   0% \n",
      "libffi-3.2.1         | 52 KB     | ########## | 100% \n",
      "\n",
      "libedit-3.1          | 171 KB    |            |   0% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "\n",
      "tk-8.6.10            | 3.2 MB    |            |   0% \n",
      "tk-8.6.10            | 3.2 MB    | #######4   |  74% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "\n",
      "ca-certificates-2020 | 128 KB    |            |   0% \n",
      "ca-certificates-2020 | 128 KB    | ########## | 100% \n",
      "\n",
      "readline-7.0         | 387 KB    |            |   0% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "\n",
      "ncurses-6.0          | 907 KB    |            |   0% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "\n",
      "sqlite-3.23.1        | 1.5 MB    |            |   0% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "\n",
      "certifi-2020.6.20    | 160 KB    |            |   0% \n",
      "certifi-2020.6.20    | 160 KB    | ########## | 100% \n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
      "\n",
      "wheel-0.35.1         | 36 KB     |            |   0% \n",
      "wheel-0.35.1         | 36 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Installing pip dependencies: ...working... \n",
      "Ran pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.3n90f1bg.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting azureml-defaults\n",
      "  Downloading azureml_defaults-1.38.0-py3-none-any.whl (3.0 kB)\n",
      "Collecting json-logging-py==0.2\n",
      "  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\n",
      "Collecting azureml-core~=1.38.0\n",
      "  Downloading azureml_core-1.38.0.post2-py3-none-any.whl (2.5 MB)\n",
      "Collecting azureml-inference-server-http~=0.4.1\n",
      "  Downloading azureml_inference_server_http-0.4.10-py3-none-any.whl (38 kB)\n",
      "Collecting azureml-dataset-runtime[fuse]~=1.38.0\n",
      "  Downloading azureml_dataset_runtime-1.38.0-py3-none-any.whl (3.5 kB)\n",
      "Collecting configparser==3.7.4\n",
      "  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting azure-mgmt-storage<20.0.0,>=16.0.0\n",
      "  Downloading azure_mgmt_storage-19.1.0-py3-none-any.whl (1.8 MB)\n",
      "Collecting contextlib2<22.0.0\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting msal<2.0.0,>=1.15.0\n",
      "  Downloading msal-1.17.0-py2.py3-none-any.whl (79 kB)\n",
      "Collecting SecretStorage<4.0.0\n",
      "  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\n",
      "Collecting msal-extensions<0.4,>=0.3.0\n",
      "  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting docker<6.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting pathspec<1.0.0\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting requests[socks]<3.0.0,>=2.19.1\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting humanfriendly<11.0,>=4.7\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting packaging<22.0,>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting ndg-httpsclient<=0.5.1\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting argcomplete<2.0\n",
      "  Downloading argcomplete-1.12.3-py2.py3-none-any.whl (38 kB)\n",
      "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting knack~=0.8.2\n",
      "  Downloading knack-0.8.2-py3-none-any.whl (59 kB)\n",
      "Collecting msrest<1.0.0,>=0.5.1\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Collecting paramiko<3.0.0,>=2.0.8\n",
      "  Downloading paramiko-2.9.2-py2.py3-none-any.whl (210 kB)\n",
      "Collecting msrestazure<=0.6.4,>=0.4.33\n",
      "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
      "Collecting urllib3<=1.26.7,>=1.23\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting azure-common<2.0.0,>=1.1.12\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting azure-mgmt-resource<21.0.0,>=15.0.0\n",
      "  Downloading azure_mgmt_resource-20.1.0-py3-none-any.whl (2.3 MB)\n",
      "Collecting azure-mgmt-containerregistry<9.0.0,>=8.2.0\n",
      "  Downloading azure_mgmt_containerregistry-8.2.0-py2.py3-none-any.whl (928 kB)\n",
      "Collecting pkginfo\n",
      "  Downloading pkginfo-1.8.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting azure-core<1.22\n",
      "  Downloading azure_core-1.21.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting azure-mgmt-authorization<1.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0\n",
      "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting PyJWT<3.0.0\n",
      "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting jsonpickle<3.0.0\n",
      "  Downloading jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyopenssl<22.0.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Collecting jmespath<1.0.0\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting adal<=1.2.7,>=1.2.0\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "Collecting azure-mgmt-keyvault<10.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-9.3.0-py2.py3-none-any.whl (412 kB)\n",
      "Collecting inference-schema==1.3.0\n",
      "  Downloading inference_schema-1.3.0-py3-none-any.whl (19 kB)\n",
      "Collecting gunicorn==20.1.0; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting applicationinsights>=0.11.7\n",
      "  Downloading applicationinsights-0.11.10-py2.py3-none-any.whl (55 kB)\n",
      "Collecting flask==1.0.3\n",
      "  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\n",
      "Collecting itsdangerous<2.0,>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pyarrow<4.0.0,>=0.17.0\n",
      "  Downloading pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7 MB)\n",
      "Collecting numpy!=1.19.3; sys_platform == \"linux\"\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting azureml-dataprep<2.27.0a,>=2.26.0a\n",
      "  Downloading azureml_dataprep-2.26.0-py3-none-any.whl (39.4 MB)\n",
      "Collecting fusepy<4.0.0,>=3.0.1; extra == \"fuse\"\n",
      "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
      "Collecting azure-mgmt-core<2.0.0,>=1.3.0\n",
      "  Downloading azure_mgmt_core-1.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jeepney>=0.6\n",
      "  Downloading jeepney-0.7.1-py3-none-any.whl (54 kB)\n",
      "Collecting portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\"\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.38.0->azureml-defaults->-r /azureml-environment-setup/condaenv.3n90f1bg.requirements.txt (line 1)) (2020.6.20)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting importlib-metadata<5,>=0.23; python_version == \"3.6\"\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.11.2-py3-none-any.whl (1.1 MB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (405 kB)\n",
      "Collecting wrapt<=1.12.1,>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=3.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gunicorn==20.1.0; platform_system != \"Windows\"->azureml-inference-server-http~=0.4.1->azureml-defaults->-r /azureml-environment-setup/condaenv.3n90f1bg.requirements.txt (line 1)) (50.3.0.post20201006)\n",
      "Collecting Jinja2>=2.10\n",
      "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting Werkzeug>=0.14\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting cloudpickle<3.0.0,>=1.1.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting azure-identity==1.7.0\n",
      "  Downloading azure_identity-1.7.0-py2.py3-none-any.whl (129 kB)\n",
      "Collecting dotnetcore2<3.0.0,>=2.1.14\n",
      "  Downloading dotnetcore2-2.1.23-py3-none-manylinux1_x86_64.whl (29.3 MB)\n",
      "Collecting azureml-dataprep-rslex~=2.2.0dev0\n",
      "  Downloading azureml_dataprep_rslex-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (13.4 MB)\n",
      "Collecting azureml-dataprep-native<39.0.0,>=38.0.0\n",
      "  Downloading azureml_dataprep_native-38.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux2010_x86_64.whl (30 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: json-logging-py, fusepy, wrapt\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3924 sha256=9718466252b1f4b5d58be630c24a5242c3fe552bc6f454ddeb6e253d7eed8856\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/1d/52/535a274b9c2ce7d4064838f2bdb62013801281ef7d7f21e2ee\n",
      "  Building wheel for fusepy (setup.py): started\n",
      "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
      "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10504 sha256=3c6d4bb16803c2e9ccfab602ece5b1de42b2f268cb2dc5e98b239eba1611f593\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69877 sha256=d83a40f374adbe67ce4239ddb08d44e3503012f5c02d4cf293a388992debc196\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built json-logging-py fusepy wrapt\n",
      "Installing collected packages: json-logging-py, urllib3, idna, charset-normalizer, PySocks, requests, six, isodate, oauthlib, requests-oauthlib, msrest, azure-core, azure-mgmt-core, azure-common, azure-mgmt-storage, contextlib2, pycparser, cffi, cryptography, PyJWT, msal, jeepney, SecretStorage, portalocker, msal-extensions, backports.weakref, backports.tempfile, websocket-client, docker, pathspec, humanfriendly, pyparsing, packaging, pyopenssl, pyasn1, ndg-httpsclient, python-dateutil, typing-extensions, zipp, importlib-metadata, argcomplete, adal, msrestazure, azure-graphrbac, pygments, pyyaml, tabulate, colorama, jmespath, knack, bcrypt, pynacl, paramiko, azure-mgmt-resource, azure-mgmt-containerregistry, pkginfo, azure-mgmt-authorization, jsonpickle, pytz, azure-mgmt-keyvault, azureml-core, wrapt, inference-schema, gunicorn, applicationinsights, MarkupSafe, Jinja2, itsdangerous, dataclasses, Werkzeug, click, flask, azureml-inference-server-http, numpy, pyarrow, cloudpickle, azure-identity, distro, dotnetcore2, azureml-dataprep-rslex, azureml-dataprep-native, azureml-dataprep, fusepy, azureml-dataset-runtime, configparser, azureml-defaults\n",
      "Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 PyJWT-2.3.0 PySocks-1.7.1 SecretStorage-3.3.1 Werkzeug-2.0.3 adal-1.2.7 applicationinsights-0.11.10 argcomplete-1.12.3 azure-common-1.1.28 azure-core-1.21.1 azure-graphrbac-0.61.1 azure-identity-1.7.0 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-8.2.0 azure-mgmt-core-1.3.0 azure-mgmt-keyvault-9.3.0 azure-mgmt-resource-20.1.0 azure-mgmt-storage-19.1.0 azureml-core-1.38.0.post2 azureml-dataprep-2.26.0 azureml-dataprep-native-38.0.0 azureml-dataprep-rslex-2.2.0 azureml-dataset-runtime-1.38.0 azureml-defaults-1.38.0 azureml-inference-server-http-0.4.10 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-3.2.0 cffi-1.15.0 charset-normalizer-2.0.12 click-8.0.4 cloudpickle-2.0.0 colorama-0.4.4 configparser-3.7.4 contextlib2-21.6.0 cryptography-36.0.1 dataclasses-0.8 distro-1.7.0 docker-5.0.3 dotnetcore2-2.1.23 flask-1.0.3 fusepy-3.0.1 gunicorn-20.1.0 humanfriendly-10.0 idna-3.3 importlib-metadata-4.8.3 inference-schema-1.3.0 isodate-0.6.1 itsdangerous-1.1.0 jeepney-0.7.1 jmespath-0.10.0 json-logging-py-0.2 jsonpickle-2.1.0 knack-0.8.2 msal-1.17.0 msal-extensions-0.3.1 msrest-0.6.21 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.19.5 oauthlib-3.2.0 packaging-21.3 paramiko-2.9.2 pathspec-0.9.0 pkginfo-1.8.2 portalocker-2.4.0 pyarrow-3.0.0 pyasn1-0.4.8 pycparser-2.21 pygments-2.11.2 pynacl-1.5.0 pyopenssl-21.0.0 pyparsing-3.0.7 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 six-1.16.0 tabulate-0.8.9 typing-extensions-4.1.1 urllib3-1.26.7 websocket-client-1.2.3 wrapt-1.12.1 zipp-3.6.0\n",
      "\n",
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0m#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "WARNING: /root/.conda/pkgs does not exist\n",
      "Removing intermediate container 901dde8ac809\n",
      " ---> 368b426e2430\n",
      "Step 9/21 : ENV PATH /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/bin:$PATH\n",
      " ---> Running in e17b259eb6c1\n",
      "Removing intermediate container e17b259eb6c1\n",
      " ---> cf9499b1b0e2\n",
      "Step 10/21 : COPY azureml-environment-setup/send_conda_dependencies.py azureml-environment-setup/send_conda_dependencies.py\n",
      " ---> 5c3bc3563311\n",
      "Step 11/21 : RUN echo \"Copying environment context\"\n",
      " ---> Running in 36efee671735\n",
      "Copying environment context\n",
      "Removing intermediate container 36efee671735\n",
      " ---> 57aefc0af72e\n",
      "Step 12/21 : COPY azureml-environment-setup/environment_context.json azureml-environment-setup/environment_context.json\n",
      " ---> 14ab55c0e557\n",
      "Step 13/21 : RUN python /azureml-environment-setup/send_conda_dependencies.py -p /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad\n",
      " ---> Running in b4c74ed026d4\n",
      "Report materialized dependencies for the environment\n",
      "Reading environment context\n",
      "Exporting conda environment\n",
      "Sending request with materialized conda environment details\n",
      "Successfully sent materialized environment details\n",
      "Removing intermediate container b4c74ed026d4\n",
      " ---> ffa06d15354f\n",
      "Step 14/21 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad\n",
      " ---> Running in bb097123ecd8\n",
      "Removing intermediate container bb097123ecd8\n",
      " ---> 22ed911d6798\n",
      "Step 15/21 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in cd1e4deb7fc9\n",
      "Removing intermediate container cd1e4deb7fc9\n",
      " ---> d32cf8d30ed0\n",
      "Step 16/21 : ENV CONDA_DEFAULT_ENV=azureml_da3e97fcb51801118b8e80207f3e01ad CONDA_PREFIX=/azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad\n",
      " ---> Running in 6b0681d6be64\n",
      "Removing intermediate container 6b0681d6be64\n",
      " ---> 05eadf909d2f\n",
      "Step 17/21 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> 0d39d1ec0308\n",
      "Step 18/21 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
      " ---> Running in c97f9d201496\n",
      "Removing intermediate container c97f9d201496\n",
      " ---> 89605acee8bc\n",
      "Step 19/21 : RUN rm -rf azureml-environment-setup\n",
      " ---> Running in 2bdac82795d6\n",
      "Removing intermediate container 2bdac82795d6\n",
      " ---> 243326e9c4d4\n",
      "Step 20/21 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 755097821a1f\n",
      "Removing intermediate container 755097821a1f\n",
      " ---> 8dbac9d355f9\n",
      "Step 21/21 : CMD [\"bash\"]\n",
      " ---> Running in 0c5bcf35a8c6\n",
      "Removing intermediate container 0c5bcf35a8c6\n",
      " ---> 3cc847fda0a1\n",
      "Successfully built 3cc847fda0a1\n",
      "Successfully tagged 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:latest\n",
      "Successfully tagged 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:1\n",
      "2022/02/23 21:06:04 Successfully executed container: acb_step_0\n",
      "2022/02/23 21:06:04 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:06:04 Pushing image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:1, attempt 1\n",
      "The push refers to repository [7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841]\n",
      "acbb037896b6: Preparing\n",
      "90b970051c68: Preparing\n",
      "5bf31f642ec0: Preparing\n",
      "f60a745d959e: Preparing\n",
      "798a2ba22de2: Preparing\n",
      "5e0f5113cc40: Preparing\n",
      "8aa51437446e: Preparing\n",
      "a9650f20fd46: Preparing\n",
      "787b410ea27b: Preparing\n",
      "5558ebbc3921: Preparing\n",
      "5a63f0a64b99: Preparing\n",
      "cfd97667b73b: Preparing\n",
      "dda643f89a6b: Preparing\n",
      "06f4d263b25f: Preparing\n",
      "5ecae586bbba: Preparing\n",
      "eb66e810a3df: Preparing\n",
      "081c0f822cac: Preparing\n",
      "0be4c2d28ad7: Preparing\n",
      "40a154bd3352: Preparing\n",
      "5e0f5113cc40: Waiting\n",
      "8aa51437446e: Waiting\n",
      "a9650f20fd46: Waiting\n",
      "787b410ea27b: Waiting\n",
      "5558ebbc3921: Waiting\n",
      "5a63f0a64b99: Waiting\n",
      "cfd97667b73b: Waiting\n",
      "dda643f89a6b: Waiting\n",
      "06f4d263b25f: Waiting\n",
      "5ecae586bbba: Waiting\n",
      "eb66e810a3df: Waiting\n",
      "081c0f822cac: Waiting\n",
      "0be4c2d28ad7: Waiting\n",
      "40a154bd3352: Waiting\n",
      "f60a745d959e: Pushed\n",
      "798a2ba22de2: Pushed\n",
      "acbb037896b6: Pushed\n",
      "90b970051c68: Pushed\n",
      "5bf31f642ec0: Pushed\n",
      "8aa51437446e: Pushed\n",
      "a9650f20fd46: Pushed\n",
      "787b410ea27b: Pushed\n",
      "5558ebbc3921: Pushed\n",
      "5a63f0a64b99: Pushed\n",
      "cfd97667b73b: Pushed\n",
      "dda643f89a6b: Pushed\n",
      "06f4d263b25f: Pushed\n",
      "081c0f822cac: Pushed\n",
      "5ecae586bbba: Pushed\n",
      "eb66e810a3df: Pushed\n",
      "40a154bd3352: Pushed\n",
      "0be4c2d28ad7: Pushed\n",
      "5e0f5113cc40: Pushed\n",
      "1: digest: sha256:e4971af48fdf72f95be9135872baf48ba9b8bbe491aababe68fcd9d3600766e7 size: 4307\n",
      "2022/02/23 21:06:56 Successfully pushed image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:1\n",
      "2022/02/23 21:06:56 Executing step ID: acb_step_2. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:06:56 Pushing image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:latest, attempt 1\n",
      "The push refers to repository [7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841]\n",
      "acbb037896b6: Preparing\n",
      "90b970051c68: Preparing\n",
      "5bf31f642ec0: Preparing\n",
      "f60a745d959e: Preparing\n",
      "798a2ba22de2: Preparing\n",
      "5e0f5113cc40: Preparing\n",
      "8aa51437446e: Preparing\n",
      "a9650f20fd46: Preparing\n",
      "787b410ea27b: Preparing\n",
      "5558ebbc3921: Preparing\n",
      "5a63f0a64b99: Preparing\n",
      "cfd97667b73b: Preparing\n",
      "dda643f89a6b: Preparing\n",
      "06f4d263b25f: Preparing\n",
      "5ecae586bbba: Preparing\n",
      "eb66e810a3df: Preparing\n",
      "081c0f822cac: Preparing\n",
      "0be4c2d28ad7: Preparing\n",
      "40a154bd3352: Preparing\n",
      "5a63f0a64b99: Waiting\n",
      "cfd97667b73b: Waiting\n",
      "dda643f89a6b: Waiting\n",
      "06f4d263b25f: Waiting\n",
      "5ecae586bbba: Waiting\n",
      "eb66e810a3df: Waiting\n",
      "5e0f5113cc40: Waiting\n",
      "081c0f822cac: Waiting\n",
      "0be4c2d28ad7: Waiting\n",
      "8aa51437446e: Waiting\n",
      "40a154bd3352: Waiting\n",
      "a9650f20fd46: Waiting\n",
      "787b410ea27b: Waiting\n",
      "5558ebbc3921: Waiting\n",
      "5bf31f642ec0: Layer already exists\n",
      "90b970051c68: Layer already exists\n",
      "798a2ba22de2: Layer already exists\n",
      "acbb037896b6: Layer already exists\n",
      "f60a745d959e: Layer already exists\n",
      "a9650f20fd46: Layer already exists\n",
      "787b410ea27b: Layer already exists\n",
      "5e0f5113cc40: Layer already exists\n",
      "5558ebbc3921: Layer already exists\n",
      "8aa51437446e: Layer already exists\n",
      "cfd97667b73b: Layer already exists\n",
      "5a63f0a64b99: Layer already exists\n",
      "dda643f89a6b: Layer already exists\n",
      "06f4d263b25f: Layer already exists\n",
      "5ecae586bbba: Layer already exists\n",
      "081c0f822cac: Layer already exists\n",
      "0be4c2d28ad7: Layer already exists\n",
      "eb66e810a3df: Layer already exists\n",
      "40a154bd3352: Layer already exists\n",
      "latest: digest: sha256:e4971af48fdf72f95be9135872baf48ba9b8bbe491aababe68fcd9d3600766e7 size: 4307\n",
      "2022/02/23 21:06:57 Successfully pushed image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_b99f1dc06f21078b9caba36d918f5841:latest\n",
      "2022/02/23 21:06:57 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 136.208997)\n",
      "2022/02/23 21:06:57 Populating digests for step ID: acb_step_0...\n",
      "2022/02/23 21:06:58 Successfully populated digests for step ID: acb_step_0\n",
      "2022/02/23 21:06:58 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 51.666827)\n",
      "2022/02/23 21:06:58 Step ID: acb_step_2 marked as successful (elapsed time in seconds: 1.459625)\n",
      "2022/02/23 21:06:58 The following dependencies were found:\n",
      "2022/02/23 21:06:58 \n",
      "- image:\n",
      "    registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "    repository: azureml/azureml_b99f1dc06f21078b9caba36d918f5841\n",
      "    tag: latest\n",
      "    digest: sha256:e4971af48fdf72f95be9135872baf48ba9b8bbe491aababe68fcd9d3600766e7\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/openmpi3.1.2-ubuntu18.04\n",
      "    tag: 20220113.v1\n",
      "    digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "  git: {}\n",
      "- image:\n",
      "    registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "    repository: azureml/azureml_b99f1dc06f21078b9caba36d918f5841\n",
      "    tag: \"1\"\n",
      "    digest: sha256:e4971af48fdf72f95be9135872baf48ba9b8bbe491aababe68fcd9d3600766e7\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/openmpi3.1.2-ubuntu18.04\n",
      "    tag: 20220113.v1\n",
      "    digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "  git: {}\n",
      "\n",
      "\n",
      "Run ID: cb1 was successful after 3m14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StepRun(prepare step) Execution Summary\n",
      "========================================\n",
      "StepRun( prepare step ) Status: Finished\n",
      "{'runId': 'cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84', 'target': 'gpu-cluster', 'status': 'Completed', 'startTimeUtc': '2022-02-23T21:15:26.433183Z', 'endTimeUtc': '2022-02-23T21:16:54.085128Z', 'services': {}, 'properties': {'ContentSnapshotId': 'f53d23bb-04fd-4725-b0f3-1f3ae8dcaf90', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '506c8be5-48d7-48af-844a-dc90de8ae3b7', 'azureml.moduleName': 'prepare step', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '48cfbf80', 'azureml.pipelinerunid': 'f329cb80-6975-47a5-915c-706f71b5f836', 'azureml.pipeline': 'f329cb80-6975-47a5-915c-706f71b5f836', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '1e75dbf8-eeb6-4b01-b461-09dda4d94f32'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'fashion_ds', 'mechanism': 'Mount'}}], 'outputDatasets': [{'identifier': {'savedId': 'e7e70bc5-8bc6-457c-b10a-8b2091a9cf89', 'registeredId': '23e5b69e-68e6-451f-8320-7642b07789de', 'registeredVersion': '1'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'output_da8e0f5a'}, 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'outputdataset/cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"e7e70bc5-8bc6-457c-b10a-8b2091a9cf89\",\n",
      "    \"name\": \"prepared_fashion_ds\",\n",
      "    \"version\": 1,\n",
      "    \"workspace\": \"Workspace.create(name='aml-workspace', subscription_id='b17f1c19-34a2-47b8-a207-40ea477828fc', resource_group='aml-resource-group')\"\n",
      "  }\n",
      "}}], 'runDefinition': {'script': 'prepare.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['DatasetConsumptionConfig:fashion_ds', 'DatasetOutputConfig:output_da8e0f5a'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster', 'dataReferences': {}, 'data': {'fashion_ds': {'dataLocation': {'dataset': {'id': '1e75dbf8-eeb6-4b01-b461-09dda4d94f32', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Mount', 'environmentVariableName': 'fashion_ds', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'output_da8e0f5a': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': 'outputdataset/{run-id}'}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'prepared_fashion_ds', 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': 'f329cb80-6975-47a5-915c-706f71b5f836', 'azureml.pipelineRun.moduleNodeId': '48cfbf80', 'azureml.pipelineRun.outputPortName': 'output_da8e0f5a'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'Experiment keras-mnist-fashion Environment', 'version': 'Autosave_2022-02-23T21:03:34Z_6b545ef0', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_da3e97fcb51801118b8e80207f3e01ad'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84/azureml-logs/20_image_build_log.txt?sv=2019-07-07&sr=b&sig=EIdgyALLbTNcmg2o%2F7nM9f5X3O2odMH2qShtdE086lM%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A03%3A49Z&se=2022-02-24T05%3A13%3A49Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=OeEcq36E3f9PNREqyzNaO2aUNH%2FhIFsSig3gZLCEZDs%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A03%3A49Z&se=2022-02-24T05%3A13%3A49Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=nrkHXkBJfrk8B5MOH8M4CU62DmN0hzcL%2BDVQ2WWsRGY%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A03%3A49Z&se=2022-02-24T05%3A13%3A49Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.cab3f93e-ecb7-4206-a1ce-3c7f7d48ca84/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=Y9AlTFhshoZw0ZDiWEtT97cxnhA1%2FXO1xRafACXppaY%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A03%3A49Z&se=2022-02-24T05%3A13%3A49Z&sp=r'}, 'submittedBy': 'Richard Vlas'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: 5f49e800-63c2-4258-b41a-b1fa7eca5b52\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/5f49e800-63c2-4258-b41a-b1fa7eca5b52?wsid=/subscriptions/b17f1c19-34a2-47b8-a207-40ea477828fc/resourcegroups/aml-resource-group/workspaces/aml-workspace&tid=0f823349-2c12-431b-a03c-b2c0a43d6fb4\n",
      "StepRun( train step ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "2022/02/23 21:17:03 Downloading source code...\n",
      "2022/02/23 21:17:03 Finished downloading source code\n",
      "2022/02/23 21:17:04 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2022/02/23 21:17:04 Successfully set up Docker network: acb_default_network\n",
      "2022/02/23 21:17:04 Setting up Docker configuration...\n",
      "2022/02/23 21:17:05 Successfully set up Docker configuration\n",
      "2022/02/23 21:17:05 Logging in to registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "2022/02/23 21:17:05 Successfully logged into 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "2022/02/23 21:17:05 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:17:05 Scanning for dependencies...\n",
      "2022/02/23 21:17:06 Successfully scanned dependencies\n",
      "2022/02/23 21:17:06 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  66.56kB\n",
      "\n",
      "Step 1/21 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf: Pulling from azureml/openmpi3.1.2-ubuntu18.04\n",
      "2f94e549220a: Already exists\n",
      "2b2d92136c10: Pulling fs layer\n",
      "0b412d2669ed: Pulling fs layer\n",
      "805003cfcee6: Pulling fs layer\n",
      "ad877dc53455: Pulling fs layer\n",
      "afa783568628: Pulling fs layer\n",
      "d67cf86adb17: Pulling fs layer\n",
      "82770c3b2808: Pulling fs layer\n",
      "c991e53ceb53: Pulling fs layer\n",
      "ad877dc53455: Waiting\n",
      "afa783568628: Waiting\n",
      "d67cf86adb17: Waiting\n",
      "82770c3b2808: Waiting\n",
      "c991e53ceb53: Waiting\n",
      "0b412d2669ed: Verifying Checksum\n",
      "0b412d2669ed: Download complete\n",
      "805003cfcee6: Verifying Checksum\n",
      "805003cfcee6: Download complete\n",
      "2b2d92136c10: Verifying Checksum\n",
      "2b2d92136c10: Download complete\n",
      "d67cf86adb17: Download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad877dc53455: Verifying Checksum\n",
      "ad877dc53455: Download complete\n",
      "82770c3b2808: Verifying Checksum\n",
      "82770c3b2808: Download complete\n",
      "c991e53ceb53: Verifying Checksum\n",
      "c991e53ceb53: Download complete\n",
      "afa783568628: Verifying Checksum\n",
      "afa783568628: Download complete\n",
      "2b2d92136c10: Pull complete\n",
      "0b412d2669ed: Pull complete\n",
      "805003cfcee6: Pull complete\n",
      "ad877dc53455: Pull complete\n",
      "afa783568628: Pull complete\n",
      "d67cf86adb17: Pull complete\n",
      "82770c3b2808: Pull complete\n",
      "c991e53ceb53: Pull complete\n",
      "Digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1@sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      " ---> 54296612fc48\n",
      "Step 2/21 : USER root\n",
      " ---> Running in 27cb60add174\n",
      "Removing intermediate container 27cb60add174\n",
      " ---> 3e55cb962061\n",
      "Step 3/21 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 990516ee4e96\n",
      "Removing intermediate container 990516ee4e96\n",
      " ---> 03177c7678ae\n",
      "Step 4/21 : WORKDIR /\n",
      " ---> Running in 8d3038f3720f\n",
      "Removing intermediate container 8d3038f3720f\n",
      " ---> ba27f510818d\n",
      "Step 5/21 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> d6e0e3a47827\n",
      "Step 6/21 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 2b06e5d1c65f\n",
      "Removing intermediate container 2b06e5d1c65f\n",
      " ---> a826ec852e20\n",
      "Step 7/21 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> d3dc1ca646d5\n",
      "Step 8/21 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in 35f91ee6a2ba\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2021 | 115 KB    |            |   0% \n",
      "ca-certificates-2021 | 115 KB    | ########## | 100% \n",
      "\n",
      "readline-7.0         | 848 KB    |            |   0% \n",
      "readline-7.0         | 848 KB    | ########## | 100% \n",
      "\n",
      "python-3.6.2         | 23.6 MB   |            |   0% \n",
      "python-3.6.2         | 23.6 MB   | ##1        |  22% \n",
      "python-3.6.2         | 23.6 MB   | ######6    |  66% \n",
      "python-3.6.2         | 23.6 MB   | ########## | 100% \n",
      "python-3.6.2         | 23.6 MB   | ########## | 100% \n",
      "\n",
      "ncurses-6.0          | 781 KB    |            |   0% \n",
      "ncurses-6.0          | 781 KB    | ########## | 100% \n",
      "ncurses-6.0          | 781 KB    | ########## | 100% \n",
      "\n",
      "xz-5.2.5             | 341 KB    |            |   0% \n",
      "xz-5.2.5             | 341 KB    | ########## | 100% \n",
      "\n",
      "libgomp-9.3.0        | 311 KB    |            |   0% \n",
      "libgomp-9.3.0        | 311 KB    | ########## | 100% \n",
      "\n",
      "libgcc-ng-9.3.0      | 4.8 MB    |            |   0% \n",
      "libgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \n",
      "libgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \n",
      "\n",
      "libstdcxx-ng-9.3.0   | 3.1 MB    |            |   0% \n",
      "libstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \n",
      "libstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \n",
      "\n",
      "sqlite-3.23.1        | 808 KB    |            |   0% \n",
      "sqlite-3.23.1        | 808 KB    | ########## | 100% \n",
      "\n",
      "libffi-3.2.1         | 48 KB     |            |   0% \n",
      "libffi-3.2.1         | 48 KB     | ########## | 100% \n",
      "\n",
      "pip-21.2.2           | 1.8 MB    |            |   0% \n",
      "pip-21.2.2           | 1.8 MB    | ########## | 100% \n",
      "pip-21.2.2           | 1.8 MB    | ########## | 100% \n",
      "\n",
      "tk-8.6.11            | 3.0 MB    |            |   0% \n",
      "tk-8.6.11            | 3.0 MB    | ########## | 100% \n",
      "tk-8.6.11            | 3.0 MB    | ########## | 100% \n",
      "\n",
      "_openmp_mutex-4.5    | 22 KB     |            |   0% \n",
      "_openmp_mutex-4.5    | 22 KB     | ########## | 100% \n",
      "\n",
      "libedit-3.1          | 151 KB    |            |   0% \n",
      "libedit-3.1          | 151 KB    | #          |  11% \n",
      "libedit-3.1          | 151 KB    | ########## | 100% \n",
      "\n",
      "zlib-1.2.11          | 108 KB    |            |   0% \n",
      "zlib-1.2.11          | 108 KB    | ########## | 100% \n",
      "\n",
      "_libgcc_mutex-0.1    | 3 KB      |            |   0% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \n",
      "\n",
      "openssl-1.0.2u       | 2.2 MB    |            |   0% \n",
      "openssl-1.0.2u       | 2.2 MB    | ########## | 100% \n",
      "openssl-1.0.2u       | 2.2 MB    | ########## | 100% \n",
      "\n",
      "wheel-0.37.1         | 33 KB     |            |   0% \n",
      "wheel-0.37.1         | 33 KB     | ########## | 100% \n",
      "\n",
      "certifi-2021.5.30    | 139 KB    |            |   0% \n",
      "certifi-2021.5.30    | 139 KB    | ########## | 100% \n",
      "\n",
      "setuptools-58.0.4    | 788 KB    |            |   0% \n",
      "setuptools-58.0.4    | 788 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Installing pip dependencies: ...working... \n",
      "Ran pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.wm2gbkdq.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting azureml-core\n",
      "  Downloading azureml_core-1.38.0.post2-py3-none-any.whl (2.5 MB)\n",
      "Collecting azureml-dataset-runtime\n",
      "  Downloading azureml_dataset_runtime-1.38.0-py3-none-any.whl (3.5 kB)\n",
      "Collecting keras==2.4.3\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting tensorflow==2.4.3\n",
      "  Downloading tensorflow-2.4.3-cp36-cp36m-manylinux2010_x86_64.whl (394.5 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/lib/python3.6/site-packages (from tensorflow==2.4.3->-r /azureml-environment-setup/condaenv.wm2gbkdq.requirements.txt (line 4)) (0.37.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting knack~=0.8.2\n",
      "  Downloading knack-0.8.2-py3-none-any.whl (59 kB)\n",
      "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting docker<6.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting azure-mgmt-keyvault<10.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-9.3.0-py2.py3-none-any.whl (412 kB)\n",
      "Collecting azure-mgmt-resource<21.0.0,>=15.0.0\n",
      "  Downloading azure_mgmt_resource-20.1.0-py3-none-any.whl (2.3 MB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0\n",
      "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "Collecting SecretStorage<4.0.0\n",
      "  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\n",
      "Collecting msal-extensions<0.4,>=0.3.0\n",
      "  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting PyJWT<3.0.0\n",
      "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting urllib3<=1.26.7,>=1.23\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting azure-common<2.0.0,>=1.1.12\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting packaging<22.0,>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting ndg-httpsclient<=0.5.1\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting azure-core<1.22\n",
      "  Downloading azure_core-1.21.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting pkginfo\n",
      "  Downloading pkginfo-1.8.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting msrestazure<=0.6.4,>=0.4.33\n",
      "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting jsonpickle<3.0.0\n",
      "  Downloading jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting adal<=1.2.7,>=1.2.0\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "Collecting msrest<1.0.0,>=0.5.1\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Collecting msal<2.0.0,>=1.15.0\n",
      "  Downloading msal-1.17.0-py2.py3-none-any.whl (79 kB)\n",
      "Collecting argcomplete<2.0\n",
      "  Downloading argcomplete-1.12.3-py2.py3-none-any.whl (38 kB)\n",
      "Collecting azure-mgmt-storage<20.0.0,>=16.0.0\n",
      "  Downloading azure_mgmt_storage-19.1.0-py3-none-any.whl (1.8 MB)\n",
      "Collecting azure-mgmt-containerregistry<9.0.0,>=8.2.0\n",
      "  Downloading azure_mgmt_containerregistry-8.2.0-py2.py3-none-any.whl (928 kB)\n",
      "Collecting jmespath<1.0.0\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting requests[socks]<3.0.0,>=2.19.1\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting contextlib2<22.0.0\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting paramiko<3.0.0,>=2.0.8\n",
      "  Downloading paramiko-2.9.2-py2.py3-none-any.whl (210 kB)\n",
      "Collecting pyopenssl<22.0.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Collecting humanfriendly<11.0,>=4.7\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pathspec<1.0.0\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting azure-mgmt-authorization<1.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\n",
      "Collecting pyarrow<4.0.0,>=0.17.0\n",
      "  Downloading pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7 MB)\n",
      "Collecting azureml-dataprep<2.27.0a,>=2.26.0a\n",
      "  Downloading azureml_dataprep-2.26.0-py3-none-any.whl (39.4 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting importlib-metadata<5,>=0.23\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting azure-mgmt-core<2.0.0,>=1.2.0\n",
      "  Downloading azure_mgmt_core-1.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting azureml-dataprep-rslex~=2.2.0dev0\n",
      "  Downloading azureml_dataprep_rslex-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (13.4 MB)\n",
      "Collecting azureml-dataprep-native<39.0.0,>=38.0.0\n",
      "  Downloading azureml_dataprep_native-38.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting cloudpickle<3.0.0,>=1.1.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting dotnetcore2<3.0.0,>=2.1.14\n",
      "  Downloading dotnetcore2-2.1.23-py3-none-manylinux1_x86_64.whl (29.3 MB)\n",
      "Collecting azure-identity==1.7.0\n",
      "  Downloading azure_identity-1.7.0-py2.py3-none-any.whl (129 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (405 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.11.2-py3-none-any.whl (1.1 MB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting portalocker<3,>=1.0\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core->-r /azureml-environment-setup/condaenv.wm2gbkdq.requirements.txt (line 1)) (2021.5.30)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting jeepney>=0.6\n",
      "  Downloading jeepney-0.7.1-py3-none-any.whl (54 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.3->-r /azureml-environment-setup/condaenv.wm2gbkdq.requirements.txt (line 4)) (58.0.4)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=b61dfc402feb8b5f53496a08d77b9d6e1d580ab65cd24b3507e5b6a9dea030cd\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69943 sha256=8a9c3d547b852860d945da40445e01a365b49539e3120fda51630ebee6b8e4bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pycparser, cffi, urllib3, PyJWT, idna, cryptography, charset-normalizer, six, requests, pyasn1, oauthlib, zipp, typing-extensions, rsa, requests-oauthlib, python-dateutil, pyasn1-modules, portalocker, msal, isodate, cachetools, msrest, msal-extensions, importlib-metadata, google-auth, distro, dataclasses, azure-core, adal, werkzeug, websocket-client, tensorboard-plugin-wit, tensorboard-data-server, tabulate, pyyaml, PySocks, pyparsing, pyopenssl, pynacl, pygments, protobuf, numpy, msrestazure, markdown, jmespath, jeepney, grpcio, google-auth-oauthlib, dotnetcore2, colorama, cloudpickle, bcrypt, backports.weakref, azureml-dataprep-rslex, azureml-dataprep-native, azure-mgmt-core, azure-identity, azure-common, argcomplete, absl-py, wrapt, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, SecretStorage, scipy, pytz, pyarrow, pkginfo, pillow, pathspec, paramiko, packaging, opt-einsum, ndg-httpsclient, knack, kiwisolver, keras-preprocessing, jsonpickle, joblib, humanfriendly, h5py, google-pasta, gast, flatbuffers, docker, cycler, contextlib2, backports.tempfile, azureml-dataprep, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, azure-graphrbac, astunparse, tensorflow, scikit-learn, pandas, matplotlib, keras, azureml-dataset-runtime, azureml-core\n",
      "Successfully installed PyJWT-2.3.0 PySocks-1.7.1 SecretStorage-3.3.1 absl-py-0.15.0 adal-1.2.7 argcomplete-1.12.3 astunparse-1.6.3 azure-common-1.1.28 azure-core-1.21.1 azure-graphrbac-0.61.1 azure-identity-1.7.0 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-8.2.0 azure-mgmt-core-1.3.0 azure-mgmt-keyvault-9.3.0 azure-mgmt-resource-20.1.0 azure-mgmt-storage-19.1.0 azureml-core-1.38.0.post2 azureml-dataprep-2.26.0 azureml-dataprep-native-38.0.0 azureml-dataprep-rslex-2.2.0 azureml-dataset-runtime-1.38.0 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-3.2.0 cachetools-4.2.4 cffi-1.15.0 charset-normalizer-2.0.12 cloudpickle-2.0.0 colorama-0.4.4 contextlib2-21.6.0 cryptography-36.0.1 cycler-0.11.0 dataclasses-0.8 distro-1.7.0 docker-5.0.3 dotnetcore2-2.1.23 flatbuffers-1.12 gast-0.3.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 humanfriendly-10.0 idna-3.3 importlib-metadata-4.8.3 isodate-0.6.1 jeepney-0.7.1 jmespath-0.10.0 joblib-1.1.0 jsonpickle-2.1.0 keras-2.4.3 keras-preprocessing-1.1.2 kiwisolver-1.3.1 knack-0.8.2 markdown-3.3.6 matplotlib-3.3.4 msal-1.17.0 msal-extensions-0.3.1 msrest-0.6.21 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.19.5 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 pandas-1.1.5 paramiko-2.9.2 pathspec-0.9.0 pillow-8.4.0 pkginfo-1.8.2 portalocker-2.4.0 protobuf-3.19.4 pyarrow-3.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pygments-2.11.2 pynacl-1.5.0 pyopenssl-21.0.0 pyparsing-3.0.7 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-0.24.2 scipy-1.5.4 six-1.15.0 tabulate-0.8.9 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.4.3 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-3.1.0 typing-extensions-3.7.4.3 urllib3-1.26.7 websocket-client-1.2.3 werkzeug-2.0.3 wrapt-1.12.1 zipp-3.6.0\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: /root/.conda/pkgs does not exist\n",
      "\n",
      "Removing intermediate container 35f91ee6a2ba\n",
      " ---> 31bc18da98ad\n",
      "Step 9/21 : ENV PATH /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/bin:$PATH\n",
      " ---> Running in 2e9a9c6db57e\n",
      "Removing intermediate container 2e9a9c6db57e\n",
      " ---> 85c471d6b4d9\n",
      "Step 10/21 : COPY azureml-environment-setup/send_conda_dependencies.py azureml-environment-setup/send_conda_dependencies.py\n",
      " ---> c3c4ee199ffd\n",
      "Step 11/21 : RUN echo \"Copying environment context\"\n",
      " ---> Running in c10f33b88f5e\n",
      "Copying environment context\n",
      "Removing intermediate container c10f33b88f5e\n",
      " ---> 369aa5aa81a5\n",
      "Step 12/21 : COPY azureml-environment-setup/environment_context.json azureml-environment-setup/environment_context.json\n",
      " ---> ca5caa7df44c\n",
      "Step 13/21 : RUN python /azureml-environment-setup/send_conda_dependencies.py -p /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657\n",
      " ---> Running in e02c1fdf8912\n",
      "Report materialized dependencies for the environment\n",
      "Reading environment context\n",
      "Exporting conda environment\n",
      "Sending request with materialized conda environment details\n",
      "Successfully sent materialized environment details\n",
      "Removing intermediate container e02c1fdf8912\n",
      " ---> 7b84b16ae87e\n",
      "Step 14/21 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657\n",
      " ---> Running in c1c4037967df\n",
      "Removing intermediate container c1c4037967df\n",
      " ---> e7591f163ffe\n",
      "Step 15/21 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in f9666b66fed1\n",
      "Removing intermediate container f9666b66fed1\n",
      " ---> 80bb13a7d5dc\n",
      "Step 16/21 : ENV CONDA_DEFAULT_ENV=azureml_f4d34986fac5bfa888344d2bd24de657 CONDA_PREFIX=/azureml-envs/azureml_f4d34986fac5bfa888344d2bd24de657\n",
      " ---> Running in 462bf125559d\n",
      "Removing intermediate container 462bf125559d\n",
      " ---> 6a27b026b9e7\n",
      "Step 17/21 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> 12dd52a3b2a0\n",
      "Step 18/21 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
      " ---> Running in 1f1ead724860\n",
      "Removing intermediate container 1f1ead724860\n",
      " ---> 22d3528d859d\n",
      "Step 19/21 : RUN rm -rf azureml-environment-setup\n",
      " ---> Running in 7f989b4f02d2\n",
      "Removing intermediate container 7f989b4f02d2\n",
      " ---> 6008afd68987\n",
      "Step 20/21 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 307d5d92b864\n",
      "Removing intermediate container 307d5d92b864\n",
      " ---> 0f15df36f3a2\n",
      "Step 21/21 : CMD [\"bash\"]\n",
      " ---> Running in 385bde4cad2d\n",
      "Removing intermediate container 385bde4cad2d\n",
      " ---> cc5fb1bd95f9\n",
      "Successfully built cc5fb1bd95f9\n",
      "Successfully tagged 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:latest\n",
      "Successfully tagged 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:1\n",
      "2022/02/23 21:19:50 Successfully executed container: acb_step_0\n",
      "2022/02/23 21:19:50 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:19:50 Pushing image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:1, attempt 1\n",
      "The push refers to repository [7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069]\n",
      "cc3d2955b0e9: Preparing\n",
      "51301c51f9e8: Preparing\n",
      "b17f4537eb7c: Preparing\n",
      "b41a8d9ed7a8: Preparing\n",
      "8563f79c8236: Preparing\n",
      "462ec5135dbb: Preparing\n",
      "30eda044cda7: Preparing\n",
      "949fe9b13dcc: Preparing\n",
      "e55e5fac98ba: Preparing\n",
      "e2ab47779f55: Preparing\n",
      "5a63f0a64b99: Preparing\n",
      "cfd97667b73b: Preparing\n",
      "dda643f89a6b: Preparing\n",
      "06f4d263b25f: Preparing\n",
      "5ecae586bbba: Preparing\n",
      "eb66e810a3df: Preparing\n",
      "081c0f822cac: Preparing\n",
      "0be4c2d28ad7: Preparing\n",
      "40a154bd3352: Preparing\n",
      "462ec5135dbb: Waiting\n",
      "30eda044cda7: Waiting\n",
      "949fe9b13dcc: Waiting\n",
      "5a63f0a64b99: Waiting\n",
      "cfd97667b73b: Waiting\n",
      "e55e5fac98ba: Waiting\n",
      "dda643f89a6b: Waiting\n",
      "06f4d263b25f: Waiting\n",
      "e2ab47779f55: Waiting\n",
      "5ecae586bbba: Waiting\n",
      "eb66e810a3df: Waiting\n",
      "40a154bd3352: Waiting\n",
      "081c0f822cac: Waiting\n",
      "0be4c2d28ad7: Waiting\n",
      "b41a8d9ed7a8: Pushed\n",
      "51301c51f9e8: Pushed\n",
      "8563f79c8236: Pushed\n",
      "cc3d2955b0e9: Pushed\n",
      "b17f4537eb7c: Pushed\n",
      "30eda044cda7: Pushed\n",
      "949fe9b13dcc: Pushed\n",
      "e55e5fac98ba: Pushed\n",
      "e2ab47779f55: Pushed\n",
      "5a63f0a64b99: Pushed\n",
      "cfd97667b73b: Pushed\n",
      "dda643f89a6b: Pushed\n",
      "06f4d263b25f: Pushed\n",
      "081c0f822cac: Pushed\n",
      "eb66e810a3df: Pushed\n",
      "40a154bd3352: Pushed\n",
      "5ecae586bbba: Pushed\n",
      "0be4c2d28ad7: Pushed\n",
      "462ec5135dbb: Pushed\n",
      "1: digest: sha256:dd5a19151cb86921ab54b59bb91609acb89417920b343fbb0ecb937bf6d8cf37 size: 4307\n",
      "2022/02/23 21:21:36 Successfully pushed image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:1\n",
      "2022/02/23 21:21:36 Executing step ID: acb_step_2. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2022/02/23 21:21:36 Pushing image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:latest, attempt 1\n",
      "The push refers to repository [7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069]\n",
      "cc3d2955b0e9: Preparing\n",
      "51301c51f9e8: Preparing\n",
      "b17f4537eb7c: Preparing\n",
      "b41a8d9ed7a8: Preparing\n",
      "8563f79c8236: Preparing\n",
      "462ec5135dbb: Preparing\n",
      "30eda044cda7: Preparing\n",
      "949fe9b13dcc: Preparing\n",
      "e55e5fac98ba: Preparing\n",
      "e2ab47779f55: Preparing\n",
      "5a63f0a64b99: Preparing\n",
      "cfd97667b73b: Preparing\n",
      "dda643f89a6b: Preparing\n",
      "06f4d263b25f: Preparing\n",
      "5ecae586bbba: Preparing\n",
      "eb66e810a3df: Preparing\n",
      "081c0f822cac: Preparing\n",
      "0be4c2d28ad7: Preparing\n",
      "40a154bd3352: Preparing\n",
      "5a63f0a64b99: Waiting\n",
      "cfd97667b73b: Waiting\n",
      "dda643f89a6b: Waiting\n",
      "06f4d263b25f: Waiting\n",
      "5ecae586bbba: Waiting\n",
      "eb66e810a3df: Waiting\n",
      "081c0f822cac: Waiting\n",
      "0be4c2d28ad7: Waiting\n",
      "40a154bd3352: Waiting\n",
      "30eda044cda7: Waiting\n",
      "949fe9b13dcc: Waiting\n",
      "e55e5fac98ba: Waiting\n",
      "e2ab47779f55: Waiting\n",
      "462ec5135dbb: Waiting\n",
      "51301c51f9e8: Layer already exists\n",
      "b17f4537eb7c: Layer already exists\n",
      "b41a8d9ed7a8: Layer already exists\n",
      "8563f79c8236: Layer already exists\n",
      "cc3d2955b0e9: Layer already exists\n",
      "30eda044cda7: Layer already exists\n",
      "462ec5135dbb: Layer already exists\n",
      "949fe9b13dcc: Layer already exists\n",
      "e55e5fac98ba: Layer already exists\n",
      "e2ab47779f55: Layer already exists\n",
      "5a63f0a64b99: Layer already exists\n",
      "cfd97667b73b: Layer already exists\n",
      "dda643f89a6b: Layer already exists\n",
      "5ecae586bbba: Layer already exists\n",
      "06f4d263b25f: Layer already exists\n",
      "eb66e810a3df: Layer already exists\n",
      "40a154bd3352: Layer already exists\n",
      "081c0f822cac: Layer already exists\n",
      "0be4c2d28ad7: Layer already exists\n",
      "latest: digest: sha256:dd5a19151cb86921ab54b59bb91609acb89417920b343fbb0ecb937bf6d8cf37 size: 4307\n",
      "2022/02/23 21:21:37 Successfully pushed image: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io/azureml/azureml_847c614cf03da7f549469ae348794069:latest\n",
      "2022/02/23 21:21:37 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 164.651438)\n",
      "2022/02/23 21:21:37 Populating digests for step ID: acb_step_0...\n",
      "2022/02/23 21:21:38 Successfully populated digests for step ID: acb_step_0\n",
      "2022/02/23 21:21:38 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 105.535771)\n",
      "2022/02/23 21:21:38 Step ID: acb_step_2 marked as successful (elapsed time in seconds: 1.510574)\n",
      "2022/02/23 21:21:38 The following dependencies were found:\n",
      "2022/02/23 21:21:38 \n",
      "- image:\n",
      "    registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "    repository: azureml/azureml_847c614cf03da7f549469ae348794069\n",
      "    tag: latest\n",
      "    digest: sha256:dd5a19151cb86921ab54b59bb91609acb89417920b343fbb0ecb937bf6d8cf37\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/openmpi3.1.2-ubuntu18.04\n",
      "    tag: 20220113.v1\n",
      "    digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "  git: {}\n",
      "- image:\n",
      "    registry: 7c6c6d46888f4b2a96b85d797d2663d3.azurecr.io\n",
      "    repository: azureml/azureml_847c614cf03da7f549469ae348794069\n",
      "    tag: \"1\"\n",
      "    digest: sha256:dd5a19151cb86921ab54b59bb91609acb89417920b343fbb0ecb937bf6d8cf37\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/openmpi3.1.2-ubuntu18.04\n",
      "    tag: 20220113.v1\n",
      "    digest: sha256:024c1f016bc4fe902601239d41f526ea987816ba25b524c22c4cb3cdd8db6ebf\n",
      "  git: {}\n",
      "\n",
      "\n",
      "Run ID: cb2 was successful after 4m37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StepRun(train step) Execution Summary\n",
      "======================================\n",
      "StepRun( train step ) Status: Finished\n",
      "{'runId': '5f49e800-63c2-4258-b41a-b1fa7eca5b52', 'target': 'gpu-cluster', 'status': 'Completed', 'startTimeUtc': '2022-02-23T21:22:02.01712Z', 'endTimeUtc': '2022-02-23T21:25:14.094486Z', 'services': {}, 'properties': {'ContentSnapshotId': 'f53d23bb-04fd-4725-b0f3-1f3ae8dcaf90', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '05fccfd1-52e9-40fe-a1c5-ed8715fdffd4', 'azureml.moduleName': 'train step', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '0cf4bb29', 'azureml.pipelinerunid': 'f329cb80-6975-47a5-915c-706f71b5f836', 'azureml.pipeline': 'f329cb80-6975-47a5-915c-706f71b5f836', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '56114f5f-e138-4efb-842a-850cc9ab7086'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'prepared_fashion_ds', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'train.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['DatasetConsumptionConfig:prepared_fashion_ds'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster', 'dataReferences': {}, 'data': {'prepared_fashion_ds': {'dataLocation': {'dataset': {'id': '56114f5f-e138-4efb-842a-850cc9ab7086', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'prepared_fashion_ds', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': 2592000, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'keras-env', 'version': 'Autosave_2022-02-23T21:16:58Z_6448ce50', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', {'pip': ['azureml-core', 'azureml-dataset-runtime', 'keras==2.4.3', 'tensorflow==2.4.3', 'numpy', 'scikit-learn', 'pandas', 'matplotlib']}], 'name': 'azureml_f4d34986fac5bfa888344d2bd24de657'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/azureml-logs/20_image_build_log.txt?sv=2019-07-07&sr=b&sig=zjqqeY2KH%2BNhihAaJkxgRhLiTpYrO%2BYY9WCdPp4PiMY%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A12%3A06Z&se=2022-02-24T05%3A22%3A06Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=SfK9LSmh3LfKoJ4q07uLdO2L1aYXVs9u%2BKFD0txeWtk%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=D2zb%2F69AarkvoyMgzzSJfZ2q8JsiJvfMPjlbibKl6hk%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=HfUof8lqa3oyFv0gQJIkm4BZvh982gRhdc1mpTjPjx8%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Nl9edoPHfGDR9yZ%2BFpy9vH5EizalxKV8akl72Zmu1I4%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=YoP3jse2NEdgfe1XdJFAxOvNcgK9ZGD7YIz00C5y57Q%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=SSFm9G0k36yQN11UPOhog8MJd0SuaOGsQ7ke3mxC1K0%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A13%3A03Z&se=2022-02-24T05%3A23%3A03Z&sp=r'}, 'submittedBy': 'Richard Vlas'}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'f329cb80-6975-47a5-915c-706f71b5f836', 'status': 'Completed', 'startTimeUtc': '2022-02-23T21:03:30.329117Z', 'endTimeUtc': '2022-02-23T21:25:15.565318Z', 'services': {}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.f329cb80-6975-47a5-915c-706f71b5f836/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=NxX19VxS2eViqjmlVuIeBwD%2BiqZ8sMr4RfASTAveG%2FM%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A15%3A18Z&se=2022-02-24T05%3A25%3A18Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.f329cb80-6975-47a5-915c-706f71b5f836/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=sPbx9Pa0A5IfWZg8qe6A5NdiP0oBo1uSyaRlhWtPyq0%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A15%3A18Z&se=2022-02-24T05%3A25%3A18Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkspace9701288440.blob.core.windows.net/azureml/ExperimentRun/dcid.f329cb80-6975-47a5-915c-706f71b5f836/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=wZpFhvnTFk8yUaaSqIAND94ynPuIp4Q2X%2F%2FGkkrccMM%3D&skoid=3b150a62-1fca-407e-bcea-d858e3318ef8&sktid=0f823349-2c12-431b-a03c-b2c0a43d6fb4&skt=2022-02-23T18%3A50%3A14Z&ske=2022-02-25T03%3A00%3A14Z&sks=b&skv=2019-07-07&st=2022-02-23T21%3A15%3A18Z&se=2022-02-24T05%3A25%3A18Z&sp=r'}, 'submittedBy': 'Richard Vlas'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gather": {
     "logged": 1618858385645
    },
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': [0.8906879425048828,\n",
       "  0.5625571608543396,\n",
       "  0.492699533700943,\n",
       "  0.44690972566604614,\n",
       "  0.4149888753890991,\n",
       "  0.3932421803474426,\n",
       "  0.3724280893802643,\n",
       "  0.3515191376209259,\n",
       "  0.33642470836639404,\n",
       "  0.33063381910324097],\n",
       " 'Accuracy': [0.6644274592399597,\n",
       "  0.7859039902687073,\n",
       "  0.8146941065788269,\n",
       "  0.8341107368469238,\n",
       "  0.8454482555389404,\n",
       "  0.8537282347679138,\n",
       "  0.8626554012298584,\n",
       "  0.8705336451530457,\n",
       "  0.875756025314331,\n",
       "  0.879527747631073],\n",
       " 'Final test loss': 0.27688536047935486,\n",
       " 'Final test accuracy': 0.9011366367340088,\n",
       " 'Loss v.s. Accuracy': 'aml://artifactId/ExperimentRun/dcid.5f49e800-63c2-4258-b41a-b1fa7eca5b52/Loss v.s. Accuracy_1645651505.png'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.find_step_run(\"train step\")[0].get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the input dataset and the output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Machine Learning dataset makes it easy to trace how your data is used in ML.\n",
    "For each Machine Learning experiment, you can easily trace the datasets used as the input through `Run` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "gather": {
     "logged": 1618858389559
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnist-fashion/t10k-images-idx3-ubyte',\n",
       " '/mnist-fashion/t10k-labels-idx1-ubyte',\n",
       " '/mnist-fashion/train-images-idx3-ubyte',\n",
       " '/mnist-fashion/train-labels-idx1-ubyte']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get input datasets\n",
    "prep_step = run.find_step_run(\"prepare step\")[0]\n",
    "inputs = prep_step.get_details()[\"inputDatasets\"]\n",
    "input_dataset = inputs[0][\"dataset\"]\n",
    "\n",
    "# list the files referenced by input_dataset\n",
    "input_dataset.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the input Fashion MNIST dataset with the workspace so that you can reuse it in other experiments or share it with your colleagues who have access to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "gather": {
     "logged": 1618858389826
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"https://data4mldemo6150520719.blob.core.windows.net/demo/mnist-fashion\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"812f759f-8813-4b5a-8e34-d1548620a2b2\",\n",
       "    \"name\": \"fashion_ds\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"image and label files from fashion mnist\",\n",
       "    \"workspace\": \"Workspace.create(name='aml-workspace', subscription_id='b17f1c19-34a2-47b8-a207-40ea477828fc', resource_group='aml-resource-group')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_ds = input_dataset.register(\n",
    "    workspace=workspace,\n",
    "    name=\"fashion_ds\",\n",
    "    description=\"image and label files from fashion mnist\",\n",
    "    create_new_version=True,\n",
    ")\n",
    "fashion_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the output model with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618858392541
    }
   },
   "outputs": [],
   "source": [
    "run.find_step_run(\"train step\")[0].register_model(\n",
    "    model_name=\"keras-model\",\n",
    "    model_path=\"outputs/model/\",\n",
    "    datasets=[(\"train test data\", fashion_ds)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Fashion MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Datasets with ML Pipeline",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "Pipeline",
   "Estimator",
   "ScriptRun"
  ],
  "task": "Train"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
